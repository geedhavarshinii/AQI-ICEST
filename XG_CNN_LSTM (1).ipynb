{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fa84db-8ccc-4f20-8fa1-368cd765385d",
   "metadata": {},
   "source": [
    "## Combining CNN and LSTM\n",
    "### First, the CNN finds important patterns in the data, like spotting key phrases in a text. Then, the LSTM remembers these patterns over time, like remembering how the weather has been changing over the past days. Together, they become really good at understanding the data and making accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e7dc1-b3a2-4915-ae61-131c0f3ecd48",
   "metadata": {},
   "source": [
    "## How It Works Together\n",
    "### 1. CNN: Finds important features in the data.\n",
    "### 2. LSTM: Remembers these features over time.\n",
    "### 3. XGBoost: Uses the features and memory to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8daad190-552b-4fca-b000-33a8796d54b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68c752d-2e87-4470-8cd6-e003653c1ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Serial number       City             Datetime     PM2.5     PM10        NO  \\\n",
      "0              1  Ahmedabad  2015-01-29 09:00:00  0.051896  0.15735 -0.717443   \n",
      "1              2  Ahmedabad  2015-01-29 12:00:00  0.099619  0.15735 -0.741590   \n",
      "2              3  Ahmedabad  2015-01-29 13:00:00 -0.136347  0.15735 -0.747717   \n",
      "3              4  Ahmedabad  2015-01-29 14:00:00 -0.149292  0.15735 -0.745915   \n",
      "4              5  Ahmedabad  2015-01-29 15:00:00 -0.249729  0.15735 -0.753123   \n",
      "\n",
      "        NO2       NOx       NH3        CO  ...  Status      Region  \\\n",
      "0 -0.589015 -0.525303  0.112012  0.032461  ...  Active  5. Western   \n",
      "1 -0.815643 -0.641089  0.112012 -0.347962  ...  Active  5. Western   \n",
      "2 -0.922628 -0.751600  0.112012 -0.444487  ...  Active  5. Western   \n",
      "3 -0.836468 -0.678852  0.112012 -0.416097  ...  Active  5. Western   \n",
      "4 -0.908745 -0.740493  0.112012 -0.529656  ...  Active  5. Western   \n",
      "\n",
      "     Day_period    Month  Year     Season Weekday_or_weekend  \\\n",
      "0    1. Morning  01. Jan  2015  1. Winter            Weekday   \n",
      "1  2. Afternoon  01. Jan  2015  1. Winter            Weekday   \n",
      "2  2. Afternoon  01. Jan  2015  1. Winter            Weekday   \n",
      "3  2. Afternoon  01. Jan  2015  1. Winter            Weekday   \n",
      "4  2. Afternoon  01. Jan  2015  1. Winter            Weekday   \n",
      "\n",
      "  Regular_day_or_holiday AQ_Acceptability Month_encoded  \n",
      "0            Regular day     Unacceptable             0  \n",
      "1            Regular day     Unacceptable             0  \n",
      "2            Regular day     Unacceptable             0  \n",
      "3            Regular day     Unacceptable             0  \n",
      "4            Regular day     Unacceptable             0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "X_train shape: (2517657, 17)\n",
      "X_test shape: (1078996, 17)\n",
      "y_train shape: (2517657,)\n",
      "y_test shape: (1078996,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\visha\\Desktop\\processed_data2.csv\")\n",
    "print(df.head())\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Region'] = label_encoder.fit_transform(df['Region'])\n",
    "df['Day_period'] = label_encoder.fit_transform(df['Day_period'])\n",
    "df['Season'] = label_encoder.fit_transform(df['Season'])\n",
    "df['Weekday_or_weekend'] = label_encoder.fit_transform(df['Weekday_or_weekend'])\n",
    "df['Regular_day_or_holiday'] = label_encoder.fit_transform(df['Regular_day_or_holiday'])\n",
    "\n",
    "# Define features and target including encoded columns\n",
    "features = df[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene',\n",
    "               'Region', 'Day_period', 'Month_encoded', 'Season', 'Weekday_or_weekend', 'Regular_day_or_holiday']]\n",
    "target = df['AQI']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the splits to ensure consistency\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7a8b3-5751-450e-886a-0408c3bde3a6",
   "metadata": {},
   "source": [
    "#### Why Use Nested 1D Array?\n",
    "#### Convolutional Operation: A 1D CNN applies convolutional filters that slide across the sequence of features (columns) within each sample (row).\n",
    "\n",
    "#### Feature Extraction: By reshaping the data into this nested 1D array format, you enable the CNN to extract meaningful patterns and relationships from the sequence of features.\n",
    "\n",
    "#### So, when you say the data is converted to a nested 1D array for a 1D CNN, it means restructuring the data to facilitate the application of convolutional operations over the sequence of features within each sample. This reshaping is crucial for leveraging the capabilities of 1D CNNs in tasks like time series analysis, where understanding patterns within sequences of data is essential.\n",
    "\n",
    "#### Splitting only the features to 1D array and CNN works with 1D data\n",
    "\n",
    "\n",
    "#### X_1d_cnn:\n",
    "#### [\n",
    "  ### [ [30], [50], [20] ],\n",
    "  ### [ [35], [55], [22] ],\n",
    "  ### [ [40], [60], [25] ],\n",
    "  ### [ [45], [65], [28] ],\n",
    "  ### [ [50], [70], [30] ]\n",
    "#### ]\n",
    "Each element [30], [50], [20], etc., represents a scalar value (feature) within the reshaped array.\n",
    "The outermost brackets [] represent the samples (days).\n",
    "The middle brackets [] represent the features within each sample.\n",
    "The innermost brackets [] represent the channel dimension added by np.expand_dims()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b38ac88-98ca-4c07-bfa5-172d95f39335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_cnn shape: (2517657, 17, 1)\n",
      "X_test_cnn shape: (1078996, 17, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reshape data for CNN (samples, timesteps, features)\n",
    "X_train_cnn = np.expand_dims(X_train.values, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test.values, axis=2)\n",
    "print(f\"X_train_cnn shape: {X_train_cnn.shape}\")\n",
    "print(f\"X_test_cnn shape: {X_test_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d580e-e6f0-4478-9a2b-d984b5375123",
   "metadata": {},
   "source": [
    "### Epoch Loop:\n",
    "\n",
    "    1. The model will loop through 50 epochs.\n",
    "    2. For each epoch:\n",
    "           The training data is divided into trainsize/32 batches of size 32.\n",
    "           The epochs is (dataset-testdataset-validationdataset)/32\n",
    "           For each batch:\n",
    "               The model performs a forward pass to make predictions.\n",
    "               The loss is calculated by comparing the predictions to the actual values.\n",
    "               The model performs a backward pass to update its weights based on the loss.\n",
    "           After processing all batches, the model's performance is evaluated on the validation set which is a part of training dataset. I have mentioned the percentage split for validation dataset as 0.3 which means 30 percent of training dataset will be for validation.\n",
    "        The training loss and validation loss are recorded and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985a026-0399-438b-b6f1-5cb5eeb338ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m853s\u001b[0m 15ms/step - loss: 0.2125 - val_loss: 0.1548\n",
      "Epoch 2/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m848s\u001b[0m 15ms/step - loss: 0.1464 - val_loss: 0.1342\n",
      "Epoch 3/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m776s\u001b[0m 14ms/step - loss: 0.1290 - val_loss: 0.1229\n",
      "Epoch 4/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 11ms/step - loss: 0.1204 - val_loss: 0.1176\n",
      "Epoch 5/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 9ms/step - loss: 0.1147 - val_loss: 0.1133\n",
      "Epoch 6/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 9ms/step - loss: 0.1105 - val_loss: 0.1098\n",
      "Epoch 7/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 9ms/step - loss: 0.1073 - val_loss: 0.1094\n",
      "Epoch 8/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 13ms/step - loss: 0.1047 - val_loss: 0.1055\n",
      "Epoch 9/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 10ms/step - loss: 0.1033 - val_loss: 0.1051\n",
      "Epoch 10/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 8ms/step - loss: 0.1015 - val_loss: 0.1025\n",
      "Epoch 11/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 7ms/step - loss: 0.1006 - val_loss: 0.1055\n",
      "Epoch 12/50\n",
      "\u001b[1m55074/55074\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 9ms/step - loss: 0.0998 - val_loss: 0.1012\n",
      "Epoch 13/50\n",
      "\u001b[1m23548/55074\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6:40\u001b[0m 13ms/step - loss: 0.0989"
     ]
    }
   ],
   "source": [
    "# Initialize the CNN-LSTM model\n",
    "cnn_lstm_model = Sequential()\n",
    "cnn_lstm_model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))\n",
    "cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_lstm_model.add(LSTM(50, return_sequences=False))\n",
    "cnn_lstm_model.add(Dense(50, activation='relu'))\n",
    "cnn_lstm_model.add(Dense(1))\n",
    "# Compile the model\n",
    "cnn_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "cnn_lstm_model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f71ec3-746c-46bf-b266-fefdd2e9f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the CNN-LSTM model\n",
    "intermediate_layer_model = Sequential()\n",
    "intermediate_layer_model.add(cnn_lstm_model.layers[0])\n",
    "intermediate_layer_model.add(cnn_lstm_model.layers[1])\n",
    "intermediate_layer_model.add(cnn_lstm_model.layers[2])\n",
    "\n",
    "X_train_features = intermediate_layer_model.predict(X_train_cnn)\n",
    "X_test_features = intermediate_layer_model.predict(X_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f9966-6e1f-41ea-8286-d19fd62188a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b27375-0fe4-4ebe-8e57-aa7bbea76ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test_features)\n",
    "\n",
    "# Calculate and print metrics for the combined model\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mape_xgb = mean_absolute_percentage_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = sqrt(mse_xgb)\n",
    "\n",
    "print(\"Combined CNN-LSTM + XGBoost Metrics:\")\n",
    "print(\"R² (Coefficient of Determination):\", r2_xgb)\n",
    "print(\"MSE (Mean Squared Error):\", mse_xgb)\n",
    "print(\"MAE (Mean Absolute Error):\", mae_xgb)\n",
    "print(\"MAPE (Mean Absolute Percentage Error):\", mape_xgb)\n",
    "print(\"RMSE (Root Mean Square Error):\", rmse_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e9a56-1386-45d9-8f93-a87750be1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate predictions for the test set using the XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test_features)\n",
    "\n",
    "# Plotting the actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_xgb, alpha=0.5, label='Predicted vs Actual', color='blue')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', label='Ideal fit')\n",
    "plt.xlabel('Actual AQI')\n",
    "plt.ylabel('Predicted AQI')\n",
    "plt.title('Actual vs Predicted AQI using CNN-LSTM + XGBoost')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609d0ab-7692-4077-ac24-d6f43b4a18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_pred_xgb has been generated using the XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test_features)\n",
    "\n",
    "# Number of points to sample for visualization\n",
    "num_points = 500\n",
    "\n",
    "# Ensure the number of points does not exceed the test set size\n",
    "num_points = min(num_points, len(y_test))\n",
    "\n",
    "# Randomly select indices for the sample\n",
    "sample_indices = np.random.choice(len(y_test), num_points, replace=False)\n",
    "\n",
    "# Sample the actual and predicted values\n",
    "y_test_sample = y_test.iloc[sample_indices]\n",
    "y_pred_xgb_sample = y_pred_xgb[sample_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot actual vs predicted values for the sampled subset\n",
    "plt.scatter(y_test_sample, y_pred_xgb_sample, color='blue', edgecolor='k', alpha=0.7, label='Predicted Values')\n",
    "\n",
    "# Ideal fit line\n",
    "plt.plot([y_test_sample.min(), y_test_sample.max()], [y_test_sample.min(), y_test_sample.max()], 'r--', lw=2, label='Ideal Fit')\n",
    "\n",
    "# Labels, title, and legend\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values using CNN-LSTM + XGBoost')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Adjusting the scale to a smaller range to zoom in\n",
    "plt.xlim([0, 5])\n",
    "plt.ylim([0, 5])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
